# k8s日志收集

## 1.1 哪些日志需要收集

![image.png](https://i.loli.net/2021/07/26/nY7CvD14yO5L2dA.png)

## 1.2 日志收集工具

- 传统架构的ELK：Elasticsearch+Logstash+Kibana
- 对于k8s采用EFK：Elasticsearch+Fluentd+Kibana

![image.png](https://i.loli.net/2021/07/26/wRNasG3ZopIxH5h.png)

## 1.3 EFK架构解析

fluented容器启动通过hostpath的方式挂载节点的`/var/log`或者其他日志路径，输出到Elasticsearch，最后通过Kibana展示

![image.png](https://i.loli.net/2021/07/26/Bw2Ujnag5XscWPq.png)

## 1.4 使用EFK收集控制台日志

部署最新版可以在k8sgithub仓库中找到：[链接](https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch)，需要将镜像同步到国内或者内网。

#### 1.4.1 下载所需资源文件

```bash
git clone https://github.com/dotbalo/k8s.git
```

```
cd 	k8s/efk-7.10.2/
```

#### 1.4.2 创建EFK所用的命名空间

```bash
kubectl create -f create-logging-namespace.yaml
```

#### 1.4.3 创建Elasticsearch集群

```bash
kubectl create -f es-service.yaml
```

```bash
kubectl create -f es-statefulset.yaml
```

#### 1.4.4 创建Kibana

```bash
kubectl create -f kibana-deployment.yaml -f kibana-service.yaml
```

#### 1.4.5 将需要采集的k8s节点打标签

由于在Kubernetes集群中，我们可能并不需要对所有的机器都采集日志，所以可以更改Fluentd的部署文件如下，添加一个NodeSelector，只部署至需要采集的主机即可

```bash
grep "nodeSelector" fluentd-es-ds.yaml -A 3
      nodeSelector:
        fluentd: "true"
	 ...
```

需要采集的k8s节点打标签

```bash
kubectl label node k8s-node01 k8s-node02 fluentd=true
```

#### 1.4.6 创建Fluentd

```bash
kubectl create -f fluentd-es-ds.yaml -f fluentd-es-configmap.yaml
```

Fluentd的ConfigMap有个字段需要注意，在**fluentd-es-configmap.yaml**最后有一个output.conf

```yaml
output.conf: |-
    <match **>
      ...
      host elasticsearch-logging #输出到es的service名，K8s内部可以直接解析
      port 9200
	 ...
```

## 1.5 Filebeat架构

filebeat以sidecar的形式和业务应用运行在同一个Pod内，使用emptyDir进行日志文件的共享，读取到日志数据输出到kafka指定topic，logstash从kafka指定topic消费日志数据并输出到elasticsearch，最后kibana展示数据

![image.png](https://i.loli.net/2021/07/26/ia2sFd9Zl8wVbk4.png)

## 1.6 使用Filebeat收集自定义文件日志

### 1.6.1 创建Kafka和Logstash

首先需要部署Kafka和Logstash至Kubernetes集群，如果企业内已经有比较成熟的技术栈，可以无需部署，直接将Filebeat的输出指向外部Kafka集群即可

```bash
cd k8s/efk-7.10.2/filebeat
```

```bash
helm install zookeeper zookeeper/ -n logging
```

```bash
helm  install kafka kafka/ -n logging
```

### 1.6.2 创建logstash

待Pod都正常后，创建Logstash服务

```bash
kubectl create -f logstash-service.yaml -f logstash-cm.yaml -f logstash.yaml -n logging
```

注意**logstash-cm.yaml**文件中的一些配置：

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: logstash-configmap
data:
  logstash.yml: |
    http.host: "0.0.0.0"
    path.config: /usr/share/logstash/pipeline
  logstash.conf: |
    # all input will come from filebeat, no local logs
    input {
      kafka {
              enable_auto_commit => true
              auto_commit_interval_ms => "1000"
              bootstrap_servers => "kafka:9092"
              topics => ["filebeat-sidecar"]
              type => ["filebeat-sidecar"]
              codec => json
          }
    }
    output {
       stdout{ codec=>rubydebug}
       if [type] == "filebeat-sidecar"{
           elasticsearch {
             hosts => ["elasticsearch-logging-0.elasticsearch-logging:9200","elasticsearch-logging-1.elasticsearch-logging:9200"]
             index => "filebeat-%{+YYYY.MM.dd}"
          }
       } else{
          elasticsearch {
             hosts => ["elasticsearch-logging-0.elasticsearch-logging:9200","elasticsearch-logging-1.elasticsearch-logging:9200"]
             index => "other-input-%{+YYYY.MM.dd}"
          }
       }
    }
```

- input：数据来源，本次示例配置的是Kakfa
- input.kafka.bootstrap_servers：Kafka地址，由于是安装在集群内部的，可以直接使用Kafka集群的Service接口，如果是外部地址，按需配置即可
- input.kafka.topics：Kafka的topic，需要和Filebeat输出的topic一致
- input.kafka.type：定义一个type，可以用于logstash输出至不同的Elasticsearch集群
- output：数据输出至哪里，本次示例输出至Elasticsearch集群，在里面配置了一个判断语句，当type为filebeat-sidecar时，将会输出至Elasticsearch集群，并且index为filebeat-xxx。

### 1.6.3 创建filebeat sidecar

添加Filebeat至该部署文件（只展示了部分内容)

在Deployment部署文件中，添加了Volumes配置，并配置了一个名为logpath的volume，将其挂载到了应用容器的**/opt/**目录和Filebeat的**/data/log/app/**目录，这样同一个Pod内的两个容器就实现了目录的共享

```yaml
...
      containers:
        - name: filebeat                        
          image: registry.cn-beijing.aliyuncs.com/dotbalo/filebeat:7.10.2 
          imagePullPolicy: IfNotPresent
          env:
            - name: podIp
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: podName
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
            - name: podNamespace
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: podDeployName
              value: app
            - name: TZ
              value: "Asia/Shanghai"
          securityContext:
            runAsUser: 0
          volumeMounts:
            - name: logpath
              mountPath: /data/log/app/
            - name: filebeatconf
              mountPath: /usr/share/filebeat/filebeat.yml 
              subPath: usr/share/filebeat/filebeat.yml
        - name: app
          image: registry.cn-beijing.aliyuncs.com/dotbalo/alpine:3.6 
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: logpath
              mountPath: /opt/
          env:
            - name: TZ
              value: "Asia/Shanghai"
            - name: LANG
              value: C.UTF-8
            - name: LC_ALL
              value: C.UTF-8
          command:
            - sh
            - -c
            - while true; do date >> /opt/date.log; sleep 2;  done 
      volumes:
        - name: logpath
          emptyDir: {}
        - name: filebeatconf
          configMap:
            name: filebeatconf
            items:
              - key: filebeat.yml
                path: usr/share/filebeat/filebeat.yml
```

### 1.6.4 创建Filebeat的配置文件

之后创建一个Filebeat的配置文件，采集该目录下的日志即可

需要注意paths是配置的共享目录，output.kafka需要和logstash的kafka为同一个集群，并且topic和logstash消费的topic为同一个。之后注入Filebeat

```yaml
# filebeat-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: filebeatconf
data:
  filebeat.yml: |-
    filebeat.inputs:
    - input_type: log
      paths:
        - /data/log/*/*.log
      tail_files: true
      fields:
        pod_name: '${podName}'
        pod_ip: '${podIp}'
        pod_deploy_name: '${podDeployName}'
        pod_namespace: '${podNamespace}'
    output.kafka:
      hosts: ["kafka:9092"]
      topic: "filebeat-sidecar"
      codec.json:
        pretty: false
      keep_alive: 30s
```

```bash
kubectl apply -f filebeat-cm.yaml -f app-filebeat.yaml -n logging
```

### 1.6.5 kibana操作

之后在Kibana上添加Filebeat的索引即可查看日志，添加步骤和EFK一致，只需要更改索引名即可

## 1.7 Loki架构

- Loki：主服务器，负责日志的存储和查询，参考了Prometheus的服务发现机制，将标签添加到日志流，而不是像其它平台一样进行全文索引；

- Promtail：负责收集日志并将其发送给Loki，主要用于发现采集目标以及添加对应Label，最终发送给Loki；

- Grafana：用来展示或查询相关日志，可以在页面查询指定标签Pod的日志。

![image.png](https://i.loli.net/2021/07/26/eJKWuiB73dkFaRb.png)

## 1.8 安装Loki

Loki提供了Helm的安装方式，可以直接下载包进行安装即可

### 1.8.1 添加并更新Loki的Helm仓库

```bash
helm repo add grafana https://grafana.github.io/helm-charts
```

```bash
helm repo update
```

### 1.8.2 创建Loki Namespace

```bash
kubectl create ns loki
```

### 1.8.3 创建Loki Stack

```bash
helm upgrade --install loki grafana/loki-stack --set grafana.enabled=true --set grafana.service.type=NodePort -n loki
```

### 1.8.4 查看Grafana密码

账户用户名默认为admin

查看密码：

```bash
kubectl get secret --namespace loki loki-grafana -o jsonpath="{.data.admin-password}" | base64 -d
```

## 1.9 Loki语法

https://grafana.com/docs/loki/latest/logql/
